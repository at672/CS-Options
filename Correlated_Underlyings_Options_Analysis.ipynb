{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Underlyings and Their effect on Option Payoffs\n",
    "\n",
    "In this exercise we will analyze three types of options with different payoff structures and determine how differences in correlation between the underlying affect the payoff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider two stocks, determined by their respective processes $\\{(S_1(t), S_2(t)) : t \\geq 0\\}$ be geometric brownian motions with drift vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "In this example, let us assume that $\\boldsymbol{\\mu} = [r, r]^T$ and, since all covariance matricies are real and positive semi-definite, $\\boldsymbol{\\Sigma}$ can be decomposed as $\\boldsymbol{\\Sigma} = AA^T$\n",
    "\n",
    "If we take $A$ to be a lower triangular matrix, then this is called a Cholesky decomposition. However, I am going to do something simpler. Since this problem only involves two variables, I will set $A = \\begin{pmatrix}\n",
    "1 & c\\\\\n",
    "c & 1\n",
    "\\end{pmatrix} $ instead of $A = \\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "c & 1\n",
    "\\end{pmatrix} $\n",
    "\n",
    "The second formulation is what would be used for a cholesky decomposition.\n",
    "\n",
    "You can verify for yourself that if we take the first definition of $A$, then the variances of each process will be equal, whereas in a cholesky decomposition, that is not necessarily true.\n",
    "\n",
    "I will choose multiple values of $c$ to control the amount of covariance that each stock has with each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the options, we will test three options. They and their payoffs are:\n",
    "\n",
    "1. Spread option: $ Y_1 = \\mathbb{E}^\\mathbb{Q} \\left[ e^{-r(T-t)}\\max(S_1(T) - S_2(T) - K, 0)  | \\mathcal{F}_t \\right] $\n",
    "2. Basket option: $ Y_1 = \\mathbb{E}^\\mathbb{Q} \\left[ e^{-r(T-t)}\\max(S_1(T) + S_2(T) - K, 0)  | \\mathcal{F}_t \\right] $\n",
    "3. Outperformance option: $ Y_1 = \\mathbb{E}^\\mathbb{Q} \\left[ e^{-r(T-t)}\\max[\\max(S_1(T) , S_2(T)) - K)]  | \\mathcal{F}_t \\right] $\n",
    "\n",
    "\n",
    "For this example, we will choose parameters as follows\n",
    "\n",
    "$S_1(0) = S_2(0) = 1, K = 2, r = 0.05, T = 1$\n",
    "$c = -0.5, 0, 0.5$\n",
    "We will use 20,000 monte carlo iterations and comment on our findings. Speaking of which, before we run any monte carlo simulations, simply by looking at the payoffs, we expect that when $S_1$ and $S_2$ diverge significantly, we expect a high payoff on the spread and a low payoff on the basket. Thus, when we believe stocks are negatively correlated, we prefer the spread option. Similarly, we expect that when $S_1$ and $S_2$ are highly correlated, we expect a high payoff on the basket and a low payoff on the spread. Thus, when we believe stocks are positively correlated, we prefer the basket option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GBM parameters\n",
    "r = 0.05\n",
    "mu = np.array([r, r])\n",
    "S0_1 = 1\n",
    "S0_2 = 1\n",
    "\n",
    "# Other parameters\n",
    "K = 2 # strike\n",
    "\n",
    "c_values = [-0.5, -0.25, 0, 0.25, 0.5]\n",
    "Sigma_values = [0, 0, 0, 0, 0]\n",
    "\n",
    "names = ['spread', 'basket', 'outperformance']\n",
    "#cart_prod = [(a,b) for a in c_values for b in names]\n",
    "\n",
    "results = {}\n",
    "for i in range(len(c_values)):\n",
    "    c = c_values[i]\n",
    "    A = np.array([[1,c],[c,1]])\n",
    "    Sigma_values[i] = np.matmul(A, np.transpose(A))\n",
    "\n",
    "#Generate 20000 sample paths\n",
    "#Note that we are generating only one time point worth of brownians. That is, in our simulation, we have a value at T = 0 and a value at T = 1, from the random variable draw\n",
    "#If we want multiple time points, we will need another dimension added to this draw\n",
    "\n",
    "np.random.seed(seed=138376)\n",
    "\n",
    "reps = 20000 # of replications (sample paths)\n",
    "T = 1\n",
    "\n",
    "#standard normal RVs for W(t)\n",
    "Z = sp.norm.rvs(loc = 0, scale = np.sqrt(T), size = (reps, 2))\n",
    "\n",
    "#WE apply the common random numbers scheme to make sure we analyze only the affects of changing the parameter c\n",
    "for i in range(0, len(Sigma_values)):\n",
    "    Sigma = Sigma_values[i]\n",
    "    c = c_values[i] #for writing to output dict\n",
    "\n",
    "    ST_1 = S0_1*np.exp((mu[0] - Sigma[0,0]/2)*T + A[0,0]*Z[:,0] + A[0,1]*Z[:,1])\n",
    "    ST_2 = S0_2*np.exp((mu[1] - Sigma[1,1]/2)*T + A[1,0]*Z[:,0] + A[1,1]*Z[:,1])\n",
    "\n",
    "    #Confidence level critical value for 95% interval\n",
    "    z_alpha2 = 1.96\n",
    "\n",
    "    payoffs_spread = np.exp(-r*T)*np.maximum(ST_1 - ST_2 - K, 0)\n",
    "    p_s_mean = np.mean(payoffs_spread)\n",
    "    p_s_std = np.std(payoffs_spread)\n",
    "    p_s_lb = p_s_mean - z_alpha2*p_s_std/np.sqrt(reps)\n",
    "    p_s_ub = p_s_mean + z_alpha2*p_s_std/np.sqrt(reps)\n",
    "    results[ (c, 'spread')] = [p_s_mean, p_s_std, p_s_lb, p_s_ub]\n",
    "\n",
    "    payoffs_basket = np.exp(-r*T)*np.maximum(ST_1 + ST_2 - K, 0);\n",
    "    p_b_mean = np.mean(payoffs_basket);\n",
    "    p_b_std = np.std(payoffs_basket);\n",
    "    p_b_lb = p_b_mean - z_alpha2*p_b_std/np.sqrt(reps)\n",
    "    p_b_ub = p_b_mean + z_alpha2*p_b_std/np.sqrt(reps)\n",
    "    results[ (c, 'basket')] = [p_b_mean, p_b_std, p_b_lb, p_b_ub]\n",
    "\n",
    "    payoffs_outperform = np.exp(-r*T)*np.maximum(np.maximum(ST_1, ST_2) - K, 0)\n",
    "    p_o_mean = np.mean(payoffs_outperform)\n",
    "    p_o_std = np.std(payoffs_outperform)\n",
    "    p_o_lb = p_o_mean - z_alpha2*p_o_std/np.sqrt(reps)\n",
    "    p_o_ub = p_o_mean + z_alpha2*p_o_std/np.sqrt(reps)\n",
    "    results[ (c, 'outperformance')] = [p_o_mean, p_o_std, p_o_lb, p_o_ub]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">-0.50</th>\n",
       "      <th colspan=\"3\" halign=\"left\">-0.25</th>\n",
       "      <th colspan=\"3\" halign=\"left\">0.00</th>\n",
       "      <th colspan=\"3\" halign=\"left\">0.25</th>\n",
       "      <th colspan=\"3\" halign=\"left\">0.50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>spread</th>\n",
       "      <th>basket</th>\n",
       "      <th>outperformance</th>\n",
       "      <th>spread</th>\n",
       "      <th>basket</th>\n",
       "      <th>outperformance</th>\n",
       "      <th>spread</th>\n",
       "      <th>basket</th>\n",
       "      <th>outperformance</th>\n",
       "      <th>spread</th>\n",
       "      <th>basket</th>\n",
       "      <th>outperformance</th>\n",
       "      <th>spread</th>\n",
       "      <th>basket</th>\n",
       "      <th>outperformance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>1.4844</td>\n",
       "      <td>15.6832</td>\n",
       "      <td>8.7895</td>\n",
       "      <td>1.7016</td>\n",
       "      <td>17.6021</td>\n",
       "      <td>9.9619</td>\n",
       "      <td>1.7799</td>\n",
       "      <td>18.2852</td>\n",
       "      <td>10.3809</td>\n",
       "      <td>1.7016</td>\n",
       "      <td>17.6021</td>\n",
       "      <td>9.9619</td>\n",
       "      <td>1.4844</td>\n",
       "      <td>15.6832</td>\n",
       "      <td>8.7895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StDev</th>\n",
       "      <td>8.2200</td>\n",
       "      <td>29.9926</td>\n",
       "      <td>19.5919</td>\n",
       "      <td>9.1007</td>\n",
       "      <td>32.9879</td>\n",
       "      <td>21.5898</td>\n",
       "      <td>9.4134</td>\n",
       "      <td>34.0494</td>\n",
       "      <td>22.2974</td>\n",
       "      <td>9.1007</td>\n",
       "      <td>32.9879</td>\n",
       "      <td>21.5898</td>\n",
       "      <td>8.2200</td>\n",
       "      <td>29.9926</td>\n",
       "      <td>19.5919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CI Lower Bound</th>\n",
       "      <td>1.3705</td>\n",
       "      <td>15.2675</td>\n",
       "      <td>8.5180</td>\n",
       "      <td>1.5755</td>\n",
       "      <td>17.1449</td>\n",
       "      <td>9.6627</td>\n",
       "      <td>1.6495</td>\n",
       "      <td>17.8133</td>\n",
       "      <td>10.0719</td>\n",
       "      <td>1.5755</td>\n",
       "      <td>17.1449</td>\n",
       "      <td>9.6627</td>\n",
       "      <td>1.3705</td>\n",
       "      <td>15.2675</td>\n",
       "      <td>8.5180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CI Upper Bound</th>\n",
       "      <td>1.5984</td>\n",
       "      <td>16.0988</td>\n",
       "      <td>9.0611</td>\n",
       "      <td>1.8278</td>\n",
       "      <td>18.0593</td>\n",
       "      <td>10.2611</td>\n",
       "      <td>1.9104</td>\n",
       "      <td>18.7571</td>\n",
       "      <td>10.6900</td>\n",
       "      <td>1.8278</td>\n",
       "      <td>18.0593</td>\n",
       "      <td>10.2611</td>\n",
       "      <td>1.5984</td>\n",
       "      <td>16.0988</td>\n",
       "      <td>9.0611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 -0.50                           -0.25           \\\n",
       "                spread   basket outperformance  spread   basket   \n",
       "Mean            1.4844  15.6832         8.7895  1.7016  17.6021   \n",
       "StDev           8.2200  29.9926        19.5919  9.1007  32.9879   \n",
       "CI Lower Bound  1.3705  15.2675         8.5180  1.5755  17.1449   \n",
       "CI Upper Bound  1.5984  16.0988         9.0611  1.8278  18.0593   \n",
       "\n",
       "                                 0.00                            0.25  \\\n",
       "               outperformance  spread   basket outperformance  spread   \n",
       "Mean                   9.9619  1.7799  18.2852        10.3809  1.7016   \n",
       "StDev                 21.5898  9.4134  34.0494        22.2974  9.1007   \n",
       "CI Lower Bound         9.6627  1.6495  17.8133        10.0719  1.5755   \n",
       "CI Upper Bound        10.2611  1.9104  18.7571        10.6900  1.8278   \n",
       "\n",
       "                                          0.50                          \n",
       "                 basket outperformance  spread   basket outperformance  \n",
       "Mean            17.6021         9.9619  1.4844  15.6832         8.7895  \n",
       "StDev           32.9879        21.5898  8.2200  29.9926        19.5919  \n",
       "CI Lower Bound  17.1449         9.6627  1.3705  15.2675         8.5180  \n",
       "CI Upper Bound  18.0593        10.2611  1.5984  16.0988         9.0611  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "asdf = pd.DataFrame(data = results)\n",
    "asdf.rename(index={0: \"Mean\", 1: \"StDev\", 2: \"CI Lower Bound\", 3: \"CI Upper Bound\"})\n",
    "\n",
    "#Output = pd.DataFrame(data = asdf.transpose(), columns = ['Mean', 'StDev', 'CI Lower Bound', 'CI Upper Bound'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we notice is that for the lowest correlation (-.50) Our "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
