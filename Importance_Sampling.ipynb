{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling for a European Call\n",
    "\n",
    "The goal of this exercise is to show how importance sampling can be used to provide a more accurate predictive interval for the price of an option."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a European call option, $ V(t, S_t) $ is defined as $ \\mathbb{E}^\\mathbb{Q} \\left[ e^{-r(T-t)}\\max(S_T - K, 0) \\right | \\mathcal{F}_t] $\n",
    "\n",
    "That is, the risk neutral expectation of the discounted terminal payoff conditional on all information obtained until the current time.\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "$S(0) = 50, \\sigma = 0.2, r = 0.06$, and $T = 0.5$\n",
    "\n",
    "Let us consider a range of strikes $K$ with values 40, 50, 70, and 90.\n",
    "\n",
    "Intuitively, we expect standard monte carlo to perform well for strikes close to the starting value, 40 and 50 in this case, and worse when the strikes are far from the current option value, in this example 70 and 90. Let's see what happens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as a reference, let's use the black scholes equations to compute the theoretical values as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11.2732', '3.5779', '0.0441', '0.0001']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import BMS\n",
    "\n",
    "\n",
    "#Define parameters\n",
    "T = 0.5\n",
    "r = 0.06\n",
    "sigma = 0.2\n",
    "S0 = 50\n",
    "Kn = np.array([40,50,70,90])\n",
    "MC = 10000 #number of monte carlo iterations.\n",
    "BS_exact = [0 for _ in range(len(Kn))]\n",
    "\n",
    "for i in range(0, len(Kn)):\n",
    "    BS_exact[i] = BMS.BMS_price('call', S = S0, K = Kn[i], r = r, q = 0, sigma = sigma, T = T, t = 0)\n",
    "\n",
    "#Format the output to 4 decimal points\n",
    "print([ '%.4f' % elem for elem in BS_exact ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's proceed with the standard Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11.3164', '3.6071', '0.0556', '0.0000']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Allocate space for storage\n",
    "MCcall = [0 for _ in range(len(Kn))]\n",
    "CIWidth_MC = [0 for _ in range(len(Kn))]\n",
    "CI_MCcall = np.zeros(shape = (len(Kn), 2))\n",
    "\n",
    "#set seed\n",
    "np.random.seed(seed=233423)\n",
    "\n",
    "#Sample from the normal distribution\n",
    "Z = scipy.stats.norm.rvs(loc = 0, scale = 1, size = MC)\n",
    "# Use standard MC method to estimate the option price\n",
    "#This is a Geoemtric brownian motion construction\n",
    "ZForMC = (r - sigma**2 / 2) * T + sigma * np.sqrt(T) * Z;\n",
    "SFinal = S0 * np.exp(ZForMC);\n",
    "for i in range(0, len(Kn)):\n",
    "    K = Kn[i]\n",
    "    #REMEMBER TO USE numpy.maximum, not np.max. They are two different functions!\n",
    "    DiscPayoff = np.exp(-r * T) * np.maximum(SFinal - K, 0)\n",
    "    MCcall[i] = np.mean(DiscPayoff)\n",
    "    StdDev = np.std(DiscPayoff)\n",
    "    #Confidence Interval\n",
    "    CI_MCcall[i,0] = MCcall[i] - 1.96*StdDev/np.sqrt(MC)\n",
    "    CI_MCcall[i,1] = MCcall[i] + 1.96*StdDev/np.sqrt(MC)\n",
    "    CIWidth_MC[i] = 1.96 * StdDev / np.sqrt(MC);\n",
    "\n",
    "print([ '%.4f' % elem for elem in MCcall ])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the standard Monte Carlo seems fairly accurate; however, on a percentage basis, we notice that the deviations for the latter 2 strikes seem to be very high.\n",
    "\n",
    "Now let's see what kind of improvements we can get with importance sampling.\n",
    "\n",
    "First, in order to perform importance sampling, we need to pick a probability density function to use in order to construct our likelihood ratio density. We want to choose a density that helps us sample from the important region. What is the important region? Intuitively, given that we are analyzing a range of strikes, and given that an option changes from valuable to having no value depending on which side of the strike it's on, we want to focus on what happens when the process $S_t$ is at or near the strike value.\n",
    "\n",
    "Thus, we want to find to choose an importance sampling distribution so that $ \\mathbb{E} \\left[S_T \\right] = K $\n",
    "\n",
    "Since we model the process $S_t$ is a Geometric Brownian Motion, which as lognormal density, we can know  $ \\mathbb{E} \\left[S_T \\right] = S(0)e^{\\mu T} = K $\n",
    "\n",
    "Every quantitiy is known here except for $\\mu$, so solving for it we have $\\mu = \\frac{1}{T} \\log \\left( \\frac{K}{S(0)} \\right) $\n",
    "\n",
    "Thus, we drift the process depending on what strike value we are investigating, which makes sense, because the further away the strike is from the intial value of our process, the greater a drift the process must have in order for the probability to be high that we \"see\" those values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for some math, We want to estimate $\\alpha$ which we define as $\\alpha = \\mathbb{E}\\left[ h(X) \\right] $ where h is our payoff function. This can be rewritten as\n",
    "\n",
    "$$\\alpha = \\mathbb{E}\\left[ e^{-rT}[S(T) - K]^+ \\right] $$\n",
    "\n",
    "If we simulate values of $S$, this is sufficient, but we can make one more simplification to make the code a bit simpler:\n",
    "$$\\alpha = \\mathbb{E}\\left[ e^{-rT}[S(0)e^X - K]^+ \\right] $$\n",
    "\n",
    "where $X \\stackrel{}{\\sim}  \\mathcal{N}((r - \\sigma^2/2)T, \\sigma^2T) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11.9689', '3.6548', '0.0432', '0.0000']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Allocate space for storage\n",
    "IScall = [0 for _ in range(len(Kn))]\n",
    "CIWidth_IS = [0 for _ in range(len(Kn))]\n",
    "CI_IScall = np.zeros(shape = (len(Kn), 2))\n",
    "\n",
    "for i in range(0, len(Kn)-1):\n",
    "    K = Kn[i]\n",
    "\n",
    "    mu = (1/T)*np.log(K/S0);\n",
    "    #Shift the normal random variables\n",
    "    ZForIS = (mu-sigma**2/2) * T + sigma*np.sqrt(T)*Z\n",
    "    ISSFinal = S0 * np.exp(ZForIS);\n",
    "    ISDiscPayoff = np.exp(-r * T) * np.maximum(ISSFinal - K, 0)\n",
    "\n",
    "    #x1 represents the drifted values taken from the original distribution\n",
    "    #x2 represents the drifted values taken from the drifted distribution\n",
    "    #dividing gives us the likelihood ratio\n",
    "    x1 = scipy.stats.norm.pdf(x = ZForIS, loc = (r - sigma**2/2)*T, scale = sigma * np.sqrt(T)) \n",
    "    x2 = scipy.stats.norm.pdf(x = ZForIS, loc = (mu - sigma**2/2)*T, scale = sigma * np.sqrt(T))\n",
    "\n",
    "    #LR = np.divide ( x1 = x1, x2 = x2) \n",
    "    LR = x1 / x2\n",
    "    ISValues = np.multiply(ISDiscPayoff , LR)\n",
    "    IScall[i] = np.mean(ISValues)\n",
    "    StdDev = np.std(ISValues)\n",
    "    CI_IScall[i,:] = [IScall[i] - 1.96*StdDev/np.sqrt(MC), IScall[i] + 1.96*StdDev/np.sqrt(MC)]\n",
    "    CIWidth_IS[i] = 1.96 * StdDev / np.sqrt(MC)\n",
    "\n",
    "print([ '%.4f' % elem for elem in IScall ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the last two values especially we see an improvement on the point estimators. If we look at confidence intervals and compute variance reduction ratios, it's even more clear. The full code, all done in one loop, is below. This is necessary because the variance reduction ratios are computed on each iteration. The only way to do this without looping through all at once would be to store every value from each individual simulation, which while possible is generally unneeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Point Estimators\n",
      "['11.3164', '3.6071', '0.0556', '0.0000']\n",
      "Monte Carlo Confidence Intervals\n",
      "[[11.17875058 11.45414765]\n",
      " [ 3.50712848  3.70698358]\n",
      " [ 0.04447797  0.06679917]\n",
      " [ 0.          0.        ]]\n",
      "Importance Sampling Point Estimators\n",
      "['11.9689', '3.6548', '0.0432', '0.0001']\n",
      "Importance Sampling Confidence Intervals\n",
      "[[1.04594278e+01 1.34784538e+01]\n",
      " [3.52332058e+00 3.78635127e+00]\n",
      " [4.21820526e-02 4.42788671e-02]\n",
      " [8.16526086e-05 8.66027566e-05]]\n",
      "Variance Reduction Ratios\n",
      "['0.0083', '0.5773', '113.3222', '0.0000']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Allocate space for storage\n",
    "MCcall = [0 for _ in range(len(Kn))]\n",
    "CIWidth_MC = [0 for _ in range(len(Kn))]\n",
    "CI_MCcall = np.zeros(shape = (len(Kn), 2))\n",
    "\n",
    "IScall = [0 for _ in range(len(Kn))]\n",
    "CIWidth_IS = [0 for _ in range(len(Kn))]\n",
    "CI_IScall = np.zeros(shape = (len(Kn), 2))\n",
    "\n",
    "variance_reduction = [0 for _ in range(len(Kn))]\n",
    "\n",
    "\n",
    "#set seed\n",
    "np.random.seed(seed=233423)\n",
    "\n",
    "#Sample from the normal distribution\n",
    "Z = scipy.stats.norm.rvs(loc = 0, scale = 1, size = MC)\n",
    "# Use standard MC method to estimate the option price\n",
    "#This is a Geoemtric brownian motion construction\n",
    "ZForMC = (r - sigma**2 / 2) * T + sigma * np.sqrt(T) * Z;\n",
    "SFinal = S0 * np.exp(ZForMC);\n",
    "for i in range(0, len(Kn)):\n",
    "    K = Kn[i]\n",
    "    ## Standard Monte Carlo\n",
    "    #REMEMBER TO USE numpy.maximum, not np.max. They are two different functions!\n",
    "    DiscPayoff = np.exp(-r * T) * np.maximum(SFinal - K, 0)\n",
    "    MCcall[i] = np.mean(DiscPayoff)\n",
    "    StdDev = np.std(DiscPayoff)\n",
    "    #Confidence Interval\n",
    "    CI_MCcall[i,0] = MCcall[i] - 1.96*StdDev/np.sqrt(MC)\n",
    "    CI_MCcall[i,1] = MCcall[i] + 1.96*StdDev/np.sqrt(MC)\n",
    "    CIWidth_MC[i] = 1.96 * StdDev / np.sqrt(MC);\n",
    "\n",
    "    #Importance Sampling\n",
    "\n",
    "    mu = (1/T)*np.log(K/S0);\n",
    "    #Shift the normal random variables\n",
    "    ZForIS = (mu-sigma**2/2) * T + sigma*np.sqrt(T)*Z\n",
    "    ISSFinal = S0 * np.exp(ZForIS);\n",
    "    ISDiscPayoff = np.exp(-r * T) * np.maximum(ISSFinal - K, 0)\n",
    "\n",
    "    #x1 represents the drifted values taken from the original distribution\n",
    "    #x2 represents the drifted values taken from the drifted distribution\n",
    "    #dividing gives us the likelihood ratio\n",
    "    x1 = scipy.stats.norm.pdf(x = ZForIS, loc = (r - sigma**2/2)*T, scale = sigma * np.sqrt(T)) \n",
    "    x2 = scipy.stats.norm.pdf(x = ZForIS, loc = (mu - sigma**2/2)*T, scale = sigma * np.sqrt(T))\n",
    "\n",
    "    #LR = np.divide ( x1 = x1, x2 = x2) \n",
    "    LR = x1 / x2\n",
    "    ISValues = np.multiply(ISDiscPayoff , LR)\n",
    "    IScall[i] = np.mean(ISValues)\n",
    "    StdDev = np.std(ISValues)\n",
    "    CI_IScall[i,:] = [IScall[i] - 1.96*StdDev/np.sqrt(MC), IScall[i] + 1.96*StdDev/np.sqrt(MC)]\n",
    "    CIWidth_IS[i] = 1.96 * StdDev / np.sqrt(MC)\n",
    "\n",
    "    #Variance Reduction from MC to IS\n",
    "    variance_reduction[i] = (np.std(DiscPayoff)/np.std(ISValues))**2\n",
    "\n",
    "print(\"Monte Carlo Point Estimators\")\n",
    "print([ '%.4f' % elem for elem in MCcall ])\n",
    "print(\"Monte Carlo Confidence Intervals\")\n",
    "print(CI_MCcall)\n",
    "\n",
    "print(\"Importance Sampling Point Estimators\")\n",
    "print([ '%.4f' % elem for elem in IScall ])\n",
    "print(\"Importance Sampling Confidence Intervals\")\n",
    "print(CI_IScall)\n",
    "\n",
    "print(\"Variance Reduction Ratios\")\n",
    "print([ '%.4f' % elem for elem in variance_reduction ])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the variance reduction ratio is invalid for the last case. We can clearly see an appreciable improvement for the strike at 70."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
